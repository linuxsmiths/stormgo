#!/usr/bin/env python3

import sys, os, csv, setproctitle
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), '..', 'common'))

import subprocess
import pytz

from helpers import *
import config as cfg

import nselib
import json

#
# This is the pylive cron program. It must be run after market close once
# everyday. It does the following:
# 1. Updates historical data so that we have latest historical data till the
#    last trading day.
# 2. After completing historical data download, it runs pyprocess to updated
#    the finalized/aggregated data. This would ensure that we have uptodate
#    daily (or lesser frequency) aggregate data needed for analysis.
# 3. For each stock, it generates the prelive data.
#    prelive data for $stock is stored in the following file
#    $tld/NSE/historical/$stock/$stock.prelive.csv
#    and it contains the previous 1 day's 1Min candles.
#    This will be used by pyprocess (live) to generate the live.xMin.csv
#    files, where x can be 1, 5, 15.
#
#    Q: Why we need previous 1 day's 1Min candles?
#    A: We have some intraday aggregates (see aggregates_i in pyprocess) that
#       we may use for analysis. This max intraday aggregate is 20 period and
#       since the max intraday candle we maintain aggregate for is 15Min, 1
#       days's worth of previous data would be enough to correctly calculate
#       all the intraday aggregates.
#
#       Note: This will result in duplicate aggregates which would be dropped
#             by the engine when updating live historical data.
#

def cleanup_stale_data():
    ''' Cleanup previous day's data files to avoid any kind of confusion.
        These are primarily the files which need to be cleaned:
        1. $stock.prelive.csv
           These are generated by this program and it correctly overwrites
           previous file, but if it doesn't run to completion we may have
           some old and some new prelive.csv files, so delete before starting
           to avoid confusion.
        2. $stock.live.csv
           These are not generated by this program but by the pylive/broker/main.py
           program.
           We need to delete these as $stock.live.csv only makes sense with
           the corresponding $stock.prelive.csv, we don't want old live.csv
           with new prelive.csv.
           Also pylive appends to live.csv file, so we need to start clean
           every day.
        3. $stock.final.live.*.csv
           These are the live aggregate files which are generated by pyprocess
           in --live mode and engine loads the live aggregate data from these.
    '''
    assert(cfg.historicaldir[0] == "/");

    PYPInfo("Removing all prelive.csv files");
    result = subprocess.run(["find",
                             "%s" % cfg.historicaldir,
                             "-name",
                             "*.prelive.csv",
                             "-exec",
                             "rm",
                             "-vf",
                             "{}",
                             ";",
                            ],
                            timeout=60, capture_output=False, text=True, check=True)
    result.check_returncode()

    PYPInfo("Removing all live.csv files");
    result = subprocess.run(["find",
                             "%s" % cfg.historicaldir,
                             "-name",
                             "*.live.csv",
                             "-exec",
                             "rm",
                             "-vf",
                             "{}",
                             ";",
                            ],
                            timeout=60, capture_output=False, text=True, check=True)
    result.check_returncode()

    PYPInfo("Removing all final.live.*.csv files");
    result = subprocess.run(["find",
                             "%s" % cfg.historicaldir,
                             "-name",
                             "*.final.live.*.csv",
                             "-exec",
                             "rm",
                             "-vf",
                             "{}",
                             ";",
                            ],
                            timeout=60, capture_output=False, text=True, check=True)
    result.check_returncode()

def backup_stale_data():
    ''' Backup previous day's prelive, live and final.live data files to:
        1. Avoid mixing up with today's files.
        2. For later analysis.

        See cleanup_stale_data() for more information about what all files are
        backed up.
    '''
    assert(cfg.historicaldir[0] == "/");

    assert(os.path.exists(cfg.logdir))
    assert(os.path.isdir(cfg.logdir))

    #
    # First time when you run with this change this assert will fire.
    # Create the backupdir_marker file by hand and retry. The assert is useful
    # and I don't want to remove it.
    #
    backupdir_marker = os.path.join(cfg.logdir, "backupdir_marker")
    assert(os.path.exists(backupdir_marker))
    assert(os.path.isfile(backupdir_marker))

    with open(backupdir_marker) as f:
        # readline() adds a '\n' in the end, strip it.
        backupdir = f.readline().rstrip()

    # Must have been created by pylive/cron/run.sh
    assert(os.path.exists(backupdir))
    assert(os.path.isdir(backupdir))

    #
    # Separate directories for prelive.csv, live.csv and final.live.XMin.csv
    # files for all stocks.
    #
    prelive_backupdir = os.path.join(backupdir, "prelive")
    live_backupdir = os.path.join(backupdir, "live")
    final_live_backupdir = os.path.join(backupdir, "final.live")

    #
    # None or all must be present.
    #
    if not os.path.exists(prelive_backupdir):
        os.makedirs(prelive_backupdir)
        os.makedirs(live_backupdir)
        os.makedirs(final_live_backupdir)


    PYPInfo("Backing up all prelive.csv files in %s" % prelive_backupdir);
    result = subprocess.run(["find",
                             "%s" % cfg.historicaldir,
                             "-name",
                             "*.prelive.csv",
                             "-exec",
                             "mv",
                             "-vf",
                             "{}",
                             prelive_backupdir,
                             ";",
                            ],
                            timeout=60, capture_output=False, text=True, check=True)
    result.check_returncode()

    PYPInfo("Backing up all live.csv files in %s" % live_backupdir);
    result = subprocess.run(["find",
                             "%s" % cfg.historicaldir,
                             "-name",
                             "*.live.csv",
                             "-exec",
                             "mv",
                             "-vf",
                             "{}",
                             live_backupdir,
                             ";",
                            ],
                            timeout=60, capture_output=False, text=True, check=True)
    result.check_returncode()

    PYPInfo("Backing up all final.live.*.csv files in %s" % final_live_backupdir);
    result = subprocess.run(["find",
                             "%s" % cfg.historicaldir,
                             "-name",
                             "*.final.live.*.csv",
                             "-exec",
                             "mv",
                             "-vf",
                             "{}",
                             final_live_backupdir,
                             ";",
                            ],
                            timeout=60, capture_output=False, text=True, check=True)
    result.check_returncode()

def refresh_historical_data():
    ''' Run pyhistorical to refresh historical data till the last trading day.
    '''
    cwd = cfg.srcdir + "/" + "pyhistorical"
    exe = cwd + "/main.py"
    PYPLog("Downloading historical data using %s" % exe)

    #
    # Hopefully it won't take more than 2 hours to download the incremental
    # historical stock data. If this is run after a long time you may want to
    # update this.
    #
    # XXX If cfg.stocklist is one.csv then this completes deceivingly and we
    #     don't have data refreshed as expected. When being called from cron
    #     there's no reason for us to use one.csv.
    #
    assert(cfg.stocklist in ["NIFTY_50.csv", "NIFTY_100.csv", "NIFTY_200.csv"])
    result = subprocess.run([exe, "--stocklistcsv", cfg.stocklist], cwd=cwd,
                            timeout=7200, capture_output=False, text=True)
    result.check_returncode()
    PYPPass("Finished downloading historical data!")

def refresh_finalized_data():
    ''' Run pyprocess to finalize the historical data to get aggregate data
        till the last trading day.
    '''
    cwd = cfg.srcdir + "/" + "pyprocess"
    exe = cwd + "/main.py"
    PYPLog("Processing historical data using %s" % exe)

    assert(cfg.stocklist in ["NIFTY_50.csv", "NIFTY_100.csv", "NIFTY_200.csv"])
    result = subprocess.run([exe, "--stocklistcsv", cfg.stocklist], cwd=cwd,
                            timeout=3600, capture_output=False, text=True)
    result.check_returncode()
    PYPPass("Finished processing historical data!")

def gen_prelive_one(stock=None):
    ''' For the given stock read 1Min candles from <stock>_<year>.partial.csv
        or if not available from <stock>_<year>.csv and create a csv file with
        name <stock>.prelive.csv containing 1Min candles for the last trading
        day.
        It assumes that refresh_historical_data() has correctly refreshed the
        historical data upto the last trading day, hence the partial.csv file
        won't be present only if it's the 1st day of the year.
    '''
    # Dir where historical csv files for this stock would be present.
    csvdir = cfg.tld + "/NSE/historical/" + stock

    now = pd.Timestamp.now()
    # Current year.
    year = now.year

    #
    # Try <stock>_<year>.partial.csv if present, else the full file.
    # Partial file if present is like INFY_2023.partial.csv.
    #
    latest_csv = ("%s/%s_%d.partial.csv" % (csvdir, stock, year))
    if not os.path.exists(latest_csv):
        # Sat/Sun + 1 extra holiday.
        assert (now.dayofyear > 3), ("%s not present, though dayofyear is %d" %
                                     (latest_csv, now.dayofyear))

        #
        # It's probably the first day of the new year, try last year's full csv file.
        # Partial file if present is like INFY_2023.partial.csv.
        # Last year's full file is like INFY_2022.csv.
        #
        latest_csv = ("%s/%s_%d.csv" % (csvdir, stock, year-1))
        if not os.path.exists(latest_csv):
            PYPError("Neither %s nor the partial file found!" % latest_csv)
            assert(False)

    PYPPass("Using file %s for prelive candles!" % latest_csv)

    dtypes = {
        'Open': np.float64,
        'High': np.float64,
        'Low': np.float64,
        'Close': np.float64,
        'Volume': np.int64
    }

    #
    # We assume columns are in the following order.
    #
    columns = ['Open', 'High', 'Low', 'Close', 'Volume']
    try:
        df = pd.read_csv(
                latest_csv,
                engine="pyarrow",
                names=columns,
                index_col=0,
                parse_dates=True,
                dtype=dtypes)
    except (ValueError, TypeError):
         PYPWarn(
             "pd.read_csv() threw ValueError/TypeError while parsing "
             "csvfile %s, most probably the first row is the header, "
             "re-trying with header=0. Consider removing the header to "
             "avoid double parsing in future!" % latest_csv)
         df = pd.read_csv(
                 latest_csv,
                 engine="pyarrow",
                 names=columns,
                 index_col=0,
                 parse_dates=True,
                 dtype=dtypes,
                 header=0)

    #
    # latest_csv has data in local timezone (09:15:00+05:30), read_csv() will
    # honour the offset but create an index with timezone set to UTC, so
    # "09:15:00+05:30" becomes "03:45:00+0000". Since we want to_csv() to dump
    # timestamp in local timezone, convet timezone to IST.
    #
    assert(df.index[0].tz == pytz.UTC)
    df.index = df.index.tz_convert("Asia/Kolkata")

    #
    # If the current time is after market hours, then we must have all the
    # 1Min candles and hence we can just look at the last candle to find out
    # which day is the last day that we want in prelive else, look for the last
    # full day 1Min data. We want the last full day 1Min data to be saved as
    # prelive.csv. That will then be loaded along with the live data for the
    # day to refresh intraday aggregates.
    #
    secs_offset = int(pd.Timestamp.now().timestamp()) % 86400
    eod_secs_offset = (15*3600 + 30*60)
    sod_secs_offset = (9*3600 + 15*60)

    PYPPass("Last candle @ %s" % df.index[-1])
    lastday = None
    if (secs_offset >= eod_secs_offset) or (secs_offset <= sod_secs_offset):
        # Last trading day.
        lastday = df.index[-1].dayofyear

        # Possibly muhurat trading data? Skip it.
        if df.index[-1].hour > 15:
            lastday = None

    if lastday is None:
        for idx in reversed(df.index):
            if idx.hour == 15 and idx.minute == 29:
                PYPPass("Using last full day candle @ %s" % idx)
                # Last full trading day.
                lastday = idx.dayofyear
                break

    assert(lastday is not None)

    # Get all rows for the last completed trading day.
    prelive_df = df[df.index.dayofyear == lastday]

    # Must have 5 columns.
    assert(prelive_df.shape[1] == 5)

    #
    # At least 350 rows. Must have 375 rows but let's leave some margin.
    #
    # XXX For some non NIFTY50 stocks (like SHREECEM) I've seen this to be so
    #     bad that one day has only ~320 candles. For now just relax this to
    #     accomdate those, but we will have to make sure we don't allow
    #     something so bad that it hurts usability of the data.
    #
    if (prelive_df.shape[0] < 220):
        PYPError("[%s] Has only %d rows (must be 375) for %s!" %
                (latest_csv, prelive_df.shape[0], df.index[-1]))
        assert False

    # Index must be of type pd.Timestamp.
    assert(prelive_df.index.inferred_type == 'datetime64')
    # Each other column must be of type float.
    assert(type(prelive_df['Open'][0]) == np.float64)
    assert(type(prelive_df['High'][0]) == np.float64)
    assert(type(prelive_df['Low'][0]) == np.float64)
    assert(type(prelive_df['Close'][0]) == np.float64)
    assert(type(prelive_df['Volume'][0]) == np.float64 or
           type(prelive_df['Volume'][0]) == np.int64)

    #
    # Dump into prelive csv file.
    # Exclude the header otherwise pyprocess (stockprocessor.py:read_ohlcv())
    # has to repeat loading csv, first time it tries w/o header and then
    # adjust to read with the first line as header.
    #
    prelive_csv = ("%s/%s.prelive.csv" % (csvdir, stock))
    prelive_df.to_csv(prelive_csv, header=False)
    PYPPass("Successfully saved prelive data in %s!" % prelive_csv)

def gen_prelive():
    #
    # Delete stale data to avoid any confusion with mixing stale and new data.
    # Note: Instead of removing we backup for later analysis.
    #
    #cleanup_stale_data();
    backup_stale_data();

    PYPLog("Generating prelive data")

    for stock in get_stocks_list():
        gen_prelive_one(stock)

    PYPPass("Done generating prelive data")

def dump_holiday_json():
    ''' Download holiday list from NSE and dump it into a json file with name
        holiday_list.nse.equities.<year>.
        C++ engine will then load holiday list from this json.
    '''
    now = pd.Timestamp.now()
    holiday_list_file = os.path.join(cfg.pylivedir, ("holiday_list.nse.equities.%d" % now.year))

    #
    # If file exists and has non-zero size, skip, else download holiday list
    # from NSE and dump into json.
    # Usually we should only download on first day of the year.
    #
    if os.path.exists(holiday_list_file):
        stat_buf = os.stat(holiday_list_file)
        holiday_list_file_size = stat_buf.st_size
        if holiday_list_file_size != 0:
            PYPPass("Holiday list file %s (size=%d) found, skipping download!" %
                    (holiday_list_file, holiday_list_file_size))
            return
        PYPWarn("Holiday list file %s is truncated, downloading fresh!" % holiday_list_file)
    else:
        PYPWarn("Holiday list file %s not found, downloading!" % holiday_list_file)

    #
    # This downloads entire holiday list for various segments.
    # We are interested only in Equities.
    # Have some reasonable sanity check asserts.
    #
    dfh = nselib.trading_holiday_calendar()
    assert(len(dfh) > 100)

    dfhe = dfh[dfh['Product'] == 'Equities'].copy()
    assert(len(dfhe) > 10)

    #
    # Holiday list returned by NSE has date in the form 26-Jan-2023, we need
    # to convert it to 2023-01-26T00:00:00 so that we can then directly pass
    # it to the BTDate constructor.
    #
    # Following assignment induces the following pandas warning:
    #
    # >>>>> snip <<<<<
    # SettingWithCopyWarning:
    # A value is trying to be set on a copy of a slice from a DataFrame.
    # Try using .loc[row_indexer,col_indexer] = value instead
    # >>>>> snip <<<<<
    #
    # The warning is thrown because the following statement adds a new column
    # 'tradingDate' to dfhe dataframe. From the usage it appears as if we
    # wanted to add this column to the original df dataframe, i.e., we indexed
    # df based on the given condition to get a ref dataframe dfhe and now in
    # that we are adding a new column so effectively we intend to add the
    # column to the original dataframe df and not to dfhe.
    # Now the problem is this - pandas does not know whether the indexing done
    # to get dfhe from df will return a view or a copy, and depending on what
    # it returns the result may be different. Today pandas may return
    # something and that may change tomorrow so the code will behave
    # differently and that's what pandas is trying to warn against.
    #
    # Since we don't care about performance here, we can explicitly call
    # copy() to make sure dfhe is a copy, now our intent is not ambiguous and
    # pandas won't complain.
    #
    # Ref:
    # https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-view-versus-copy
    # https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas
    #
    dfhe.tradingDate = dfhe.tradingDate.map(
        lambda x: pd.Timestamp(x).to_pydatetime().strftime("%Y-%m-%dT%H:%M:%S"))

    with open(holiday_list_file, "w+") as f:
        json.dump(dfhe.to_dict(), f, indent=2)

    PYPPass("Holiday list file %s created!" % holiday_list_file)

def main():
    setproctitle.setproctitle("pylive.cron");

    # Anything before this log is due to modules getting imported.
    PYPPass(('==> Starting pylive.cron... [logfile=%s]' % (cfg.logfile)), console=True)

    ts1 = pd.Timestamp.now()

    refresh_historical_data()
    ts2 = pd.Timestamp.now()
    PYPPass(('Refesh historical data took %s' % (ts2-ts1)), console=True)

    refresh_finalized_data()
    ts3 = pd.Timestamp.now()
    PYPPass(('Finalize historical data took %s' % (ts3-ts2)), console=True)

    gen_prelive()
    ts4 = pd.Timestamp.now()
    PYPPass(('Gen prelive took: %s' % (ts4-ts3)), console=True)

    # Fetch holiday list for the year from NSE if not already done.
    dump_holiday_json()
    ts5 = pd.Timestamp.now()
    PYPPass(('Dump holiday json took: %s' % (ts5-ts4)), console=True)

    PYPPass(('Done pylive.cron, took %s' % (ts5-ts1)), console=True)

if __name__ == '__main__':
    main()
